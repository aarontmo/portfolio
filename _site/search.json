[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Welcome to my corner of the web! I’m Aaron Morris, a passionate Data Scientist with a strong background in Applied Biostatistics Data Science. My journey through academia and professional experiences has molded me into a data enthusiast, adept at turning complex data into meaningful insights.\n\nMy Journey\nI hold a Master’s degree in Applied Biostatistics from Boston University and a Bachelor’s degree in Biochemistry with a minor in Statistics from Brigham Young University - Idaho. These academic experiences laid the groundwork for my expertise in data analysis, machine learning, and statistical modeling.\n\n\nMy Expertise\nOver the years, I’ve honed my skills in various aspects of data science:\nPredictive Data Modeling & Exploratory Data Analysis: I love diving deep into data to uncover patterns and build models that predict future trends (see Predicting Breast Cancer).\nData Visualization: Proficient in ggplot2, matplotlib, plotly, and seaborn, I create visuals that tell compelling stories from data.\nProgramming: I’m well-versed in Python (Pandas, Numpy, Scikit-Learn, Scikit-Image, Statsmodels, Selenium) and R, making me versatile in handling diverse data science projects.\nSQL & Jupyter Notebook: These tools are integral to my workflow, helping me manage and analyze data efficiently.\nImage Processing & Classical Image Segmentation: I have experience in processing and analyzing complex image data, particularly in the biomedical field.\nGit Source Control: Collaboration and version control are crucial in my projects, ensuring smooth teamwork and project management.\n\n\nProfessional Adventures\nElephas Biosciences\nAt Elephas Biosciences, I worked as a Data Scientist, providing analytical support for various R&D projects. My role involved quality control report automation, differential analysis, fold change analysis, and microscopy image processing. One of my proudest achievements was reducing assay time by 67% through fold change analysis on cytokine concentration levels.\nBoston University School of Public Health\nAs a Graduate Research Assistant, I delved into MRI image processing to predict brain tumor survivability based on gene mutations. I developed coefficient images using Python and Numpy, extracted quantitative features with Pyradiomics, and used machine learning models to predict gene mutations.\nIn my role as a Research Data Analyst, I supported chronic kidney disease research in Nicaragua. I computed summary statistics, built visualizations, and developed regression models to investigate relationships between health indicators and demographic traits.\nBYU-Idaho Institutional Data and Analysis Services\nHere, I transformed messy data into clean, actionable insights using R programming and the tidyverse. I created and maintained dashboards in Power BI, and automated data gathering processes, enhancing the efficiency of data management.\n\n\nBeyond Data\nWhen I’m not immersed in data, you’ll likely find me enjoying the great outdoors. I love hiking, camping, and rock climbing. I also have a passion for playing pickleball, a sport that keeps me active and competitive.\n\n\nLet’s Connect\nI believe in the power of collaboration and am always eager to connect with like-minded individuals. Feel free to explore my projects and reach out to me on LinkedIn. Let’s turn data into actionable insights together!\nContact Me:\nEmail: aaronmorris9696@gmail.com\nLinkedIn: Linkedin\nThank you for visiting my portfolio. I look forward to connecting with you!"
  },
  {
    "objectID": "breast_cancer_prediction.html",
    "href": "breast_cancer_prediction.html",
    "title": "Breast Cancer Classification",
    "section": "",
    "text": "Probelem: Automate the diagnosis of breast cancer.\nBackground: Breast cancer is one of the most prevalent forms of cancer among woment. Early detection and diagnosis (malignant or benign) is crucial for a positive response to treatment.\nGoal: The goal of the project is to train a machine learning model to predict whether breast tissue is malignant or benign. This classification will be based on several features that were extracted from microscopy images of fine needle aspirate of a breast tissue.\nData: More information on the data can be found here and here\n\n\nCode\n# import libraries\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, f1_score, classification_report\n\nfrom scipy.stats import sem\n\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\n\n\nCode\n# read in data\ndata_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\data\")\ndata = pd.read_csv(data_dir / 'breast_cancer.csv')"
  },
  {
    "objectID": "breast_cancer_prediction.html#data-preprocessing",
    "href": "breast_cancer_prediction.html#data-preprocessing",
    "title": "Breast Cancer Classification",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThree preprocessing steps are necessary before training any machine learning algorithms: feature scaling, outcome binarization, and train test split. While feature scaling is necessary for some machine learning algorithms it is less imortant for others. Machine learning algorithms that rely on measuring distances between data points and boundaries require feature scaling to ensure each feature contributes equally to the classification. KNN, logistic regression, and support vector classifier require feature scaling while decision tree and random forest classifiers do not because they are non-parametric machine learning models.\nOutcome Binarization\nMost machine learning algorithms in the Scikit-Learn library assume the outcome to be binary so we will dichotomize the diagnosis as follows:\n\nmalignant = 1\nbenign = 0\n\nSplitting Data\nBefore splitting the data we want to check if there are any missing values in any of the columns\n\n\nCode\ndata.isnull().sum()\n\n\nid                           0\ndiagnosis                    0\nradius_mean                  0\ntexture_mean                 0\nperimeter_mean               0\narea_mean                    0\nsmoothness_mean              0\ncompactness_mean             0\nconcavity_mean               0\nconcave points_mean          0\nsymmetry_mean                0\nfractal_dimension_mean       0\nradius_se                    0\ntexture_se                   0\nperimeter_se                 0\narea_se                      0\nsmoothness_se                0\ncompactness_se               0\nconcavity_se                 0\nconcave points_se            0\nsymmetry_se                  0\nfractal_dimension_se         0\nradius_worst                 0\ntexture_worst                0\nperimeter_worst              0\narea_worst                   0\nsmoothness_worst             0\ncompactness_worst            0\nconcavity_worst              0\nconcave points_worst         0\nsymmetry_worst               0\nfractal_dimension_worst      0\nUnnamed: 32                569\ndtype: int64\n\n\nIt appears there are no missing values in any features except for the last column Unnamed: 32 which are all empty. Next we will remove the empty column and the id column since it is not relevant to predicting diagnosis outcome.\n\n\nCode\nclean_data = (\n    data\n    .drop(columns=['id', 'Unnamed: 32'], axis=1)\n    .assign(diagnosis = np.where(data['diagnosis'] == 'M', 1, 0))\n)\n\n\nWe will use a 70:30 training:test split which will result in 398 training samples and 171 testing samples. We will also stratify by diagnosis to ensure we the same proportions of malignant to benign samples in the training and testing set.\n\n\nCode\n# split into features and labels\nX = clean_data.drop(columns=['diagnosis'], axis=1)\ny = clean_data[['diagnosis']]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=2024)\n\n# ensure class proportions was preserver between training and testing splits\nfig, ax = plt.subplots(1,2)\n\nsns.countplot(\n    y_train,\n    x='diagnosis',\n    ax=ax[0]\n    )\n\nax[0].set_title('Count of Training Labels')\n\nsns.countplot(\n    y_test,\n    x='diagnosis',\n    ax=ax[1]\n    )\n\nax[1].set_title('Count of Testing Labels')\n\n\nText(0.5, 1.0, 'Count of Testing Labels')\n\n\n\n\n\n\n\n\n\nThe plot above verifies that the proportion of positive and negative labels was preserved between the training and testing set.\nWe will perform the scaling before the training of the algorithms where it is required."
  },
  {
    "objectID": "breast_cancer_prediction.html#model-training",
    "href": "breast_cancer_prediction.html#model-training",
    "title": "Breast Cancer Classification",
    "section": "Model Training",
    "text": "Model Training\n\nK-Nearest Neighbors\nThe first model we will use is the K-Nearest neighbors model. This model works by assigning a label to a value based on its K nearest neighbors. The distance between points must be calculated using some distance measure. Below is an explanation of each hyperparameter. Because this model relys on distances between points we will scale the data before training.\nHyperparameters\n\nn_neighbors: The number of nearest neighbors to consider when making a prediction. Larger values lead to a more generalized prediction with the risk of underfitting the data. Small values lead to a model that is more sensitive to noise in the data potentially leading to overfitting.\nweights: Determines how the distance between the point of interest and its nearest neighbors influence the prediction. Uniform weights consider all neighbors equally when getting a prediction. With distance weights closer neighbors have a larger impact on the prediction.\nalgorithm: The algorithm that is used to compute the neighbors: brute force, ball tree, KD tree, auto\nleaf_size: Parameter passed to BallTree or KDTree algorithm, can effect the speed of the training\np: power parameter passed to the Minkowski metric. When p = 1 distance metric is city block, when p = 2 distance metric is euclidean.\n\nWe will be using a randomized search with a 5 fold cross validation to train the model and tune the hyperparameters.\n\n\nCode\nscaler = StandardScaler()\n\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\ny_train = np.array(y_train).ravel()\ny_test = np.array(y_test).ravel()\n\n# knn_param_dist = {\n#     'n_neighbors': np.arange(1,31),\n#     'weights': ['uniform', 'distance'],\n#     'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto'],\n#     'leaf_size': np.arange(10, 51, 5),\n#     'p': [1,2]\n# }\n\n# knn = KNeighborsClassifier()\n\n# knn_random_search = RandomizedSearchCV(\n#     knn, \n#     param_distributions=knn_param_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# knn_random_search.fit(x_train_scaled, y_train)\n\n\n\n\nCode\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# knn_pickle_path = ml_model_dir / 'breast_cancer_knn_model.pkl'\n# with open(knn_pickle_path, 'wb') as file:\n#     pickle.dump(knn_random_search, file)\n\n\nThe table below shows the hyperparameters that were chosen to produce the best result.\n\n\nCode\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_knn_model.pkl', 'rb') as file:\n    knn_random_search = pickle.load(file)\n\npd.DataFrame([knn_random_search.best_params_])\n\n\n\n\n\n\n\n\n\nweights\np\nn_neighbors\nleaf_size\nalgorithm\n\n\n\n\n0\ndistance\n2\n6\n20\nball_tree\n\n\n\n\n\n\n\n\n\nCode\nknn_preds = knn_random_search.predict(x_test_scaled)\nknn_probs = knn_random_search.predict_proba(x_test_scaled)[:, 1]\n\naccuracy = accuracy_score(y_test, knn_preds)\ncm = confusion_matrix(y_test, knn_preds)\n\nfpr, tpr, thresholds = roc_curve(y_test, knn_probs)\n\ntick_labels = data[['diagnosis']].drop_duplicates()\n\ndef resultsPlot(confusion_matrix, fpr, tpr):\n\n    roc_auc = auc(fpr, tpr)\n\n    fig, ax = plt.subplots(1,2, figsize=(9.5,5))\n    sns.heatmap(\n        confusion_matrix, \n        annot=True, \n        fmt='d', \n        cmap='Blues', \n        xticklabels=['B', 'M'], \n        yticklabels=['B', 'M'],\n        ax=ax[0])\n    ax[0].set_title('Confusion Matrix')\n    ax[0].set_xlabel('Predicted Label')\n    ax[0].set_ylabel('True Label')\n\n    ax[1].plot(\n        fpr,\n        tpr,\n        color='darkorange',\n        lw=2,\n        label=f'AUC = {roc_auc:.2f}'\n    )\n    ax[1].plot(\n        [0,1], \n        [0,1],\n        color='navy',\n        lw=2,\n        linestyle='--'\n    )\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')\n    ax[1].set_title('ROC Curve')\n    ax[1].legend(loc='lower right')\n\nresultsPlot(cm, fpr, tpr)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(\n    classification_report(y_test, knn_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\n\n\n\n\n1\n1.0\n0.90625\n0.95082\n\n\n\n\n\n\n\nAs shown by the validation results above, this model performed very well with an f1-score of 0.95 and AUC score of 0.997. As you can see from the confusion matrix there were 6 false negatives where the model incorrectly predicted a sample as benign when it was malignant.\n\n\nLogistic Regression\nThe next model we will use is the logistic regression. Traditionally the logistic regression returns odds of a value being positive. However we can use the logistic regression as a classifier by converting the odds into probabilities and then setting a threshold to classify the samples into two categories. Because this project is primarily concerned with prediction I won’t discuss the mathematical model and the effects of different beta coefficients.\n\n\nCode\nlr = LogisticRegression(fit_intercept=True)\n\nlr.fit(x_train_scaled, y_train)\n\nlr_probs = lr.predict_proba(x_test_scaled)[:,1]\nthreshs = np.arange(0.1,1,0.01)\n\naccs = []\nfor thresh in threshs:\n    preds = lr_probs &gt; thresh\n    acc = accuracy_score(y_test, preds)\n    accs.append(acc)\n\nacc_df = pd.DataFrame(\n    {\n        'threshs': threshs,\n        'accs': accs})\n\nplt.plot(threshs, accs)\nplt.xlabel('Thresholds')\nplt.ylabel('Accuracy')\n\n\nText(0, 0.5, 'Accuracy')\n\n\n\n\n\n\n\n\n\nAccording to the plot above the accuracy reaches a maximum at a threshold around 0.5, so we will us that as our cutoff.\n\n\nCode\nlr_preds = lr_probs &gt; 0.5\n\nfpr, tpr, threshold = roc_curve(y_test, lr_probs)\n\nlr_cm = confusion_matrix(y_test, lr_preds)\n\nresultsPlot(lr_cm, fpr, tpr)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(\n    classification_report(y_test, lr_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\n\n\n\n\n1\n1.0\n0.9375\n0.967742\n\n\n\n\n\n\n\nThe logistic regression also did a very good job with this classification task with an f1-scor of 0.97 and an AUC of 0.995. Similarly to the KNN model the logistic regression misclassified 4 malignant samples as benign.\n\n\nDecision Tree\nThe next model we will evaluate is a simple decision tree. The decision tree model works by using the features to split the samples into different buckets for classification. Due to the way a decision tree works feature scaling is not necessary for this model. The decision tree has the following hyperparameters that need to be tuned.\n\ncriterion: the function to measure the quality of the split at each node, options are gini impurity, log loss, and entropy\nsplitter: how the algorithm determines the split at each node, options are best and random\nmaximum depth: controls how deep the tree can get. The deeper the tree the higher likelihood of overfitting, the shallower the tree the higher the likelihood of underfitting.\nminimum samples splt: the minimum number of samples needed before an internal leaf node splits\nminimum samples leaf: the minimum number of samples required for a node to be a leaf node\nmax features: the number of samples that are considered for each leaf node. We will be using either log2(number of features) or sqrt(number of features).\n\nSimilar to the KNN model we will be using a randomized cross validation search for training and tuning the decision tree model.\n\n\nCode\n# dt_tune_dist = {\n#     'criterion': ['gini', 'entropy', 'log_loss'],\n#     'splitter': ['best', 'random'],\n#     'max_depth': np.arange(1,21,1),\n#     'min_samples_split': np.arange(2,21,1),\n#     'min_samples_leaf': np.arange(1,21,1),\n#     'max_features': ['log2', 'sqrt']\n# }\n\n# dt = DecisionTreeClassifier()\n\n# dt_random_search = RandomizedSearchCV(\n#     dt,\n#     param_distributions=dt_tune_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# dt_random_search.fit(x_train, y_train)\n\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# dt_pickle_path = ml_model_dir / 'breast_cancer_dt_model.pkl'\n# with open(dt_pickle_path, 'wb') as file:\n#     pickle.dump(dt_random_search, file)\n\n\n\n\nCode\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_dt_model.pkl', 'rb') as file:\n    dt_random_search = pickle.load(file)\n\n\ndt_best_model = dt_random_search.best_estimator_\ndt_feature_imp = dt_best_model.feature_importances_\n\ndt_feature_imp_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': dt_feature_imp\n}).sort_values('importance', ascending=False)\n\nsns.barplot(\n    dt_feature_imp_df,\n    y='feature',\n    x='importance'\n)   \nplt.title('Decision Tree Feature Importance')\n\n\nText(0.5, 1.0, 'Decision Tree Feature Importance')\n\n\n\n\n\n\n\n\n\nThe bar chart above shows the feature importances of the features that were used in the decision tree. The worst perimeter was the main feature that was used to make the prediction. The worst perimeter is defined at the mean of the three largest cell perimeters.\n\n\nCode\nplot_tree(\n    dt_best_model,\n    filled=False,\n    feature_names=x_train.columns,\n    rounded=True\n)\nimg_dir = Path(os.getcwd() + '/data/images')\nimg_file = 'breast_cancer_dt.png'\nplt.savefig(img_dir / img_file)\nplt.close()\n\n\nThe image below shows the actual decision tree that was used to make the predictions. Each node shows the feature that was used and the cutoff for that feature. This aids in the understanding of how decision trees work.\n\n\n\nDecision Tree Plot\n\n\n\n\nCode\ndt_preds = dt_random_search.predict(x_test)\ndt_probs = dt_random_search.predict_proba(x_test)[:, 1]\n\ndt_cm = confusion_matrix(y_test, dt_preds)\n\ndt_fpr, dt_tpr, thresholds = roc_curve(y_test, dt_probs)\n\nresultsPlot(dt_cm, dt_fpr, dt_tpr)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(\n    classification_report(y_test, dt_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\n\n\n\n\n1\n0.866667\n0.8125\n0.83871\n\n\n\n\n\n\n\nBecause of the simplicity of the decision tree model it tends not to perform quite as well as other more complex models. The model returned an f1-score of 0.84 and an AUC score of 0.93. The model misclassified 12 malignant samples as benign (false negative) and 8 benign samples as malignant (false positive).\n\n\nRandom Forest\nThe random forest model is an extension of the decision tree model. The randome forest model fits several smaller decision trees and outputting the mode prediction for each sample. There several pros and cons to decision trees; the main pro is that is more accurate that a single decision tree because it combines several weak predictors to make a more accurate prediction. One of the main cons of a random forest classifier is that the increased complexity increases the training time and the ability to interpret the model. The hyperparameters to be tuned are shown below:\n\nn_estimators: how many individual decision trees to train\ncriterion: the function to measure the quality of the split at each node, options are gini impurity, log loss, and entropy\nmaximum depth: controls how deep the tree can get. The deeper the tree the higher likelihood of overfitting, the shallower the tree the higher the likelihood of underfitting.\nminimum samples splt: the minimum number of samples needed before an internal leaf node splits\nminimum samples leaf: the minimum number of samples required for a node to be a leaf node\nmax features: the number of samples that are considered for each leaf node. We will be using either log2(number of features) or sqrt(number of features).\n\n\n\nCode\n# rf_para_dist = {\n#     'n_estimators': np.arange(1,500,1),\n#     'criterion': ['gini', 'entropy', 'log_loss'],\n#     'max_depth': np.arange(1,21,1),\n#     'min_samples_split': np.arange(2,21,1),\n#     'min_samples_leaf': np.arange(1,21,1),\n#     'max_features': ['log2', 'sqrt']\n# }\n\n# rf = RandomForestClassifier()\n\n# rf_random_search = RandomizedSearchCV(\n#     rf,\n#     param_distributions=rf_para_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# rf_random_search.fit(x_train, y_train)\n\n\n\n\nCode\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# rf_pickle_path = ml_model_dir / 'breast_cancer_rf_model.pkl'\n# with open(rf_pickle_path, 'wb') as file:\n#     pickle.dump(rf_random_search, file)\n\n\n\n\nCode\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_rf_model.pkl', 'rb') as file:\n    rf_random_search = pickle.load(file)\n\nrf_best_model = rf_random_search.best_estimator_\nrf_feature_imp = rf_best_model.feature_importances_\nrf_feature_imp_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': rf_feature_imp\n}).sort_values('importance', ascending=False)\n\nsns.barplot(\n    rf_feature_imp_df,\n    y='feature',\n    x='importance'\n)\nplt.title('Random Forest Feature Importance')\n\n\nText(0.5, 1.0, 'Random Forest Feature Importance')\n\n\n\n\n\n\n\n\n\nThe plot above shows the feature importances for all the features that were used in the predictions. An obvious difference between this model and simple decision tree is that the random forest model uses all the features.\n\n\nCode\nrf_preds = rf_random_search.predict(x_test)\nrf_cm = confusion_matrix(y_test, rf_preds)\nrf_probs = rf_random_search.predict_proba(x_train)[:, 1]\nrf_fpr, rf_tpr, thresholds = roc_curve(y_test, rf_preds)\n\n\nresultsPlot(rf_cm, rf_fpr, rf_tpr)\n\n\n\n\n\n\n\n\n\n\n\nCode\npd.DataFrame(\n    classification_report(y_test, rf_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\n\n\n\n\n1\n0.95082\n0.90625\n0.928\n\n\n\n\n\n\n\nThe random forest model returned an f1-score of 0.93 and an AUC score 0.94. There were six false negatives and 3 false positives. The following section compares the accuracy of all the models to decide on the best model."
  }
]