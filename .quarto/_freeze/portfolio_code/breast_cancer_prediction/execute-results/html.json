{
  "hash": "705198f2617018cea0b64ef708d845c5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Breast Cancer Classification\n---\n\n\n# Introduction\n\n**Probelem:** Automate the diagnosis of breast cancer.\n\n**Background:** Breast cancer is one of the most prevalent forms of cancer among woment. Early detection and diagnosis (malignant or benign) is crucial for a positive response to treatment. \n\n**Goal:** The goal of the project is to train a machine learning model to predict whether breast tissue is malignant or benign. This classification will be based on several features that were extracted from microscopy images of fine needle aspirate of a breast tissue. \n\n**Data:** More information on the data can be found [here](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/data) and [here](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)\n\n::: {#0b4ba560 .cell execution_count=1}\n``` {.python .cell-code}\n# import libraries\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, f1_score\n\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n```\n:::\n\n\n::: {#b4bee077 .cell execution_count=2}\n``` {.python .cell-code}\n# read in data\ndata_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\data\")\ndata = pd.read_csv(data_dir / 'breast_cancer.csv')\n```\n:::\n\n\n# Exploratory Data Analysis\n\nThe data include ten morphological and texture based features of cell nuclei in each image including the mean, standard error, and largest value (mean of three largest values) for each feature. The list below shows all the features that will be used for classification.\n\n- radius \n\n- texture (standard deviation of gray-scale values)\n\n- perimeter\n\n- area\n\n- smoothness\n\n- compactness\n\n- concavity\n\n- concave points\n\n- symmetry\n\n- fractal dimension\n\nWe can get a good idea of what features will be important in the classification model by looking at the distributions of each feature individually subsetted by the diagnosis. Because there are 30 feature columns we will only look at the mean of each the features above.\n\n::: {#a6d14b85 .cell execution_count=3}\n``` {.python .cell-code}\n# subset data for features ending in _mean and diagnosis\nmean_df = (\n    data\n    .filter(regex='mean|diagnosis')\n)\n\nmean_cols = [col for col in mean_df.columns if 'mean' in col]\n\nfor col in mean_cols:\n    plt.figure(figsize=(10,6))\n\n    sns.histplot(\n        mean_df,\n        x=col,\n        hue='diagnosis',\n        kde=True,\n        stat='density'\n    )\n    \n    plt.title(f'Distribution of {col}')\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-1.png){width=820 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-2.png){width=820 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-3.png){width=837 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-4.png){width=846 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-5.png){width=808 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-6.png){width=799 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-7.png){width=799 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-8.png){width=821 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-9.png){width=808 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-4-output-10.png){width=808 height=523}\n:::\n:::\n\n\nA few observations based on these plots:\n\n- It appears (with the a few exceptions) that the malignant group generally has a broader distribution comapared to the benign group which appears to have a tighter spread (more on this later).\n\n- With the exception of fractal dimension, symmetry, and smoothness there appears to be good separation between the malignant and benign distribtusions\n\n- The malignant group appears to have higher values on average compared to the benign group\n\nI am curious about the spread of the malignant and benign groups. Below is a table showing the standard deviations of the ten features.\n\n::: {#45a8b1d9 .cell execution_count=4}\n``` {.python .cell-code}\n(\n    data\n    .filter(regex='mean|diagnosis')\n    .groupby('diagnosis')\n    .agg('std')\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>fractal_dimension_mean</th>\n    </tr>\n    <tr>\n      <th>diagnosis</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>B</th>\n      <td>1.780512</td>\n      <td>3.995125</td>\n      <td>11.807438</td>\n      <td>134.287118</td>\n      <td>0.013446</td>\n      <td>0.033750</td>\n      <td>0.043442</td>\n      <td>0.015909</td>\n      <td>0.024807</td>\n      <td>0.006747</td>\n    </tr>\n    <tr>\n      <th>M</th>\n      <td>3.203971</td>\n      <td>3.779470</td>\n      <td>21.854653</td>\n      <td>367.937978</td>\n      <td>0.012608</td>\n      <td>0.053987</td>\n      <td>0.075019</td>\n      <td>0.034374</td>\n      <td>0.027638</td>\n      <td>0.007573</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFor most of the features the standard deviation of the malginant group is larger than benign group. The features where this is most apparent are the morphological features such as radius (3.20 vs 1.78) and perimeter (21.85 vs 11.81). It is generally known that cancer cells have irregular shapes and sizes some being larger and some being smaller than normal cells. This is a reasonable explanation for the wider spread of morphological features in the malignant group.\n\nOne of biggest concerns with classification tasks is how balanced the outcome variable is. The barchart below shows the total number of malignant and benign breat tissue samples.\n\n::: {#e969a88e .cell execution_count=5}\n``` {.python .cell-code}\n# diagnosis counts\nax = sns.countplot(\n    data,\n    x='diagnosis'\n)\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        p.get_x() + p.get_width() / 2,\n        height + 3,\n        f'{height:.0f}',\n        ha=\"center\",\n    )\n\nplt.title('Outcome Count')\nplt.xlabel('Diagnosis')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-6-output-1.png){width=593 height=449}\n:::\n:::\n\n\n63% of the observations are benign. The outcome is slightly unbalanced but no to the point where we would need to use any imputation  or oversampling method.\n\n# Predictive Modeling\n\nThe following five machine learning algorithms will be trained and evaluated to verify which is the most accurate for this problem:\n\n- K-Nearest Neighbors (KNN)\n\n- Logistic Regression\n\n- Support Vector Classifier (SVC)\n\n- Decision Tree Classifier\n\n- Random Forest Classifier\n\n## Data Preprocessing\n\nThree preprocessing steps are necessary before training any machine learning algorithms: feature scaling, outcome binarization, and train test split. While feature scaling is necessary for some machine learning algorithms it is less imortant for others. Machine learning algorithms that rely on measuring distances between data points and boundaries require feature scaling to ensure each feature contributes equally to the classification. KNN, logistic regression, and support vector classifier require feature scaling while decision tree and random forest classifiers do not because they are non-parametric machine learning models.\n\n**Outcome Binarization**\n\nMost machine learning algorithms in the Scikit-Learn library assume the outcome to be binary so we will dichotomize the diagnosis as follows:\n\n- malignant = 1\n\n- benign = 0\n\n**Splitting Data**\n\nBefore splitting the data we want to check if there are any missing values in any of the columns\n\n::: {#3c118d71 .cell execution_count=6}\n``` {.python .cell-code}\ndata.isnull().sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nid                           0\ndiagnosis                    0\nradius_mean                  0\ntexture_mean                 0\nperimeter_mean               0\narea_mean                    0\nsmoothness_mean              0\ncompactness_mean             0\nconcavity_mean               0\nconcave points_mean          0\nsymmetry_mean                0\nfractal_dimension_mean       0\nradius_se                    0\ntexture_se                   0\nperimeter_se                 0\narea_se                      0\nsmoothness_se                0\ncompactness_se               0\nconcavity_se                 0\nconcave points_se            0\nsymmetry_se                  0\nfractal_dimension_se         0\nradius_worst                 0\ntexture_worst                0\nperimeter_worst              0\narea_worst                   0\nsmoothness_worst             0\ncompactness_worst            0\nconcavity_worst              0\nconcave points_worst         0\nsymmetry_worst               0\nfractal_dimension_worst      0\nUnnamed: 32                569\ndtype: int64\n```\n:::\n:::\n\n\nIt appears there are no missing values in any features except for the last column `Unnamed: 32` which are all empty. Next we will remove the empty column and the id column since it is not relevant to predicting diagnosis outcome.\n\n::: {#439c98e3 .cell execution_count=7}\n``` {.python .cell-code}\nclean_data = (\n    data\n    .drop(columns=['id', 'Unnamed: 32'], axis=1)\n    .assign(diagnosis = np.where(data['diagnosis'] == 'M', 1, 0))\n)\n```\n:::\n\n\nWe will use a 70:30 training:test split which will result in 398 training samples and 171 testing samples. We will also stratify by diagnosis to ensure we the same proportions of malignant to benign samples in the training and testing set.\n\n::: {#52626f43 .cell execution_count=8}\n``` {.python .cell-code}\n# split into features and labels\nX = clean_data.drop(columns=['diagnosis'], axis=1)\ny = clean_data[['diagnosis']]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=2024)\n\n# ensure class proportions was preserver between training and testing splits\nfig, ax = plt.subplots(1,2)\n\nsns.countplot(\n    y_train,\n    x='diagnosis',\n    ax=ax[0]\n    )\n\nax[0].set_title('Count of Training Labels')\n\nsns.countplot(\n    y_test,\n    x='diagnosis',\n    ax=ax[1]\n    )\n\nax[1].set_title('Count of Testing Labels')\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nText(0.5, 1.0, 'Count of Testing Labels')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-9-output-2.png){width=593 height=449}\n:::\n:::\n\n\nThe plot above verifies that the proportion of positive and negative labels was preserved between the training and testing set.\n\nWe will perform the scaling before the training of the algorithms where it is required.\n\n## Model Training\n\n### K-Nearest Neighbors\n\nThe first model we will use is the K-Nearest neighbors model. This model works by assigning a label to a value based on its K nearest neighbors. The distance between points must be calculated using some distance measure. Below is an explanation of each hyperparameter. Because this model relys on distances between points we will scale the data before training.\n\n**Hyperparameters**\n\n- n_neighbors: The number of nearest neighbors to consider when making a prediction. Larger values lead to a more generalized prediction with the risk of underfitting the data. Small values lead to a model that is more sensitive to noise in the data potentially leading to overfitting.\n\n- weights: Determines how the distance between the point of interest and its nearest neighbors influence the prediction. Uniform weights consider all neighbors equally when getting a prediction. With distance weights closer neighbors have a larger impact on the prediction.\n\n- algorithm: The algorithm that is used to compute the neighbors: brute force, ball tree, KD tree, auto\n\n- leaf_size: Parameter passed to BallTree or KDTree algorithm, can effect the speed of the training\n\n- p: power parameter passed to the Minkowski metric. When p = 1 distance metric is city block, when p = 2 distance metric is euclidean. \n\nWe will be using a randomized search with a 5 fold cross validation to train the model and tune the hyperparameters.\n\n::: {#456b0367 .cell execution_count=9}\n``` {.python .cell-code}\nscaler = StandardScaler()\n\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\ny_train = np.array(y_train).ravel()\ny_test = np.array(y_test).ravel()\n\nknn_param_dist = {\n    'n_neighbors': np.arange(1,31),\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto'],\n    'leaf_size': np.arange(10, 51, 5),\n    'p': [1,2]\n}\n\nknn = KNeighborsClassifier()\n\nknn_random_search = RandomizedSearchCV(\n    knn, \n    param_distributions=knn_param_dist,\n    n_iter=100,\n    cv=5,\n    n_jobs=1,\n    random_state=2024\n)\n\nknn_random_search.fit(x_train_scaled, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=KNeighborsClassifier(), n_iter=100, n_jobs=1,\n                   param_distributions={&#x27;algorithm&#x27;: [&#x27;ball_tree&#x27;, &#x27;kd_tree&#x27;,\n                                                      &#x27;brute&#x27;, &#x27;auto&#x27;],\n                                        &#x27;leaf_size&#x27;: array([10, 15, 20, 25, 30, 35, 40, 45, 50]),\n                                        &#x27;n_neighbors&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]),\n                                        &#x27;p&#x27;: [1, 2],\n                                        &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]},\n                   random_state=2024)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomizedSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(cv=5, estimator=KNeighborsClassifier(), n_iter=100, n_jobs=1,\n                   param_distributions={&#x27;algorithm&#x27;: [&#x27;ball_tree&#x27;, &#x27;kd_tree&#x27;,\n                                                      &#x27;brute&#x27;, &#x27;auto&#x27;],\n                                        &#x27;leaf_size&#x27;: array([10, 15, 20, 25, 30, 35, 40, 45, 50]),\n                                        &#x27;n_neighbors&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]),\n                                        &#x27;p&#x27;: [1, 2],\n                                        &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]},\n                   random_state=2024)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: KNeighborsClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=np.int64(20),\n                     n_neighbors=np.int64(6), weights=&#x27;distance&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;KNeighborsClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(algorithm=&#x27;ball_tree&#x27;, leaf_size=np.int64(20),\n                     n_neighbors=np.int64(6), weights=&#x27;distance&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\nThe table below shows the hyperparameters that were chosen to produce the best result.\n\n::: {#9da7e761 .cell execution_count=10}\n``` {.python .cell-code}\npd.DataFrame([knn_random_search.best_params_])\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>weights</th>\n      <th>p</th>\n      <th>n_neighbors</th>\n      <th>leaf_size</th>\n      <th>algorithm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>distance</td>\n      <td>2</td>\n      <td>6</td>\n      <td>20</td>\n      <td>ball_tree</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe will be using the accuracy along with an ROC curve and AUC score to evaluate how well this model and future models perform. \n\n::: {#3d837d02 .cell execution_count=11}\n``` {.python .cell-code}\nknn_preds = knn_random_search.predict(x_test_scaled)\nknn_probs = knn_random_search.predict_proba(x_test_scaled)[:, 1]\n\naccuracy = accuracy_score(y_test, knn_preds)\ncm = confusion_matrix(y_test, knn_preds)\n\nfpr, tpr, thresholds = roc_curve(y_test, knn_probs)\n\ntick_labels = data[['diagnosis']].drop_duplicates()\n\ndef resultsPlot(confusion_matrix, fpr, tpr):\n\n    roc_auc = auc(fpr, tpr)\n\n    fig, ax = plt.subplots(1,2, figsize=(9.5,5))\n    sns.heatmap(\n        confusion_matrix, \n        annot=True, \n        fmt='d', \n        cmap='Blues', \n        xticklabels=['B', 'M'], \n        yticklabels=['B', 'M'],\n        ax=ax[0])\n    ax[0].set_title('Confusion Matrix')\n    ax[0].set_xlabel('Predicted Label')\n    ax[0].set_ylabel('True Label')\n\n    ax[1].plot(\n        fpr,\n        tpr,\n        color='darkorange',\n        lw=2,\n        label=f'AUC = {roc_auc:.2f}'\n    )\n    ax[1].plot(\n        [0,1], \n        [0,1],\n        color='navy',\n        lw=2,\n        linestyle='--'\n    )\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')\n    ax[1].set_title('ROC Curve')\n    ax[1].legend(loc='lower right')\n\nresultsPlot(cm, fpr, tpr)\n```\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-12-output-1.png){width=767 height=449}\n:::\n:::\n\n\nThis model performed very well with an accuracy of 96.5% and AUC score of 0.997. As you can see from the confusion matrix there were 6 false negatives where the model incorrectly predicted a sample as benign when it was malignant.\n\n### Logistic Regression\n\nThe next model we will use is the logistic regression. Traditionally the logistic regression returns odds of a value being positive. However we can use the logistic regression as a classifier by converting the odds into probabilities and then setting a threshold to classify the samples into two categories. Because this project is primarily concerned with prediction I won't discuss the mathematical model and the effects of different beta coefficients. \n\n::: {#b686bd0c .cell execution_count=12}\n``` {.python .cell-code}\nlr = LogisticRegression(fit_intercept=True)\n\nlr.fit(x_train_scaled, y_train)\n\nlr_probs = lr.predict_proba(x_test_scaled)[:,1]\nthreshs = np.arange(0.1,1,0.01)\n\naccs = []\nfor thresh in threshs:\n    preds = lr_probs > thresh\n    acc = accuracy_score(y_test, preds)\n    accs.append(acc)\n\nacc_df = pd.DataFrame(\n    {\n        'threshs': threshs,\n        'accs': accs})\n\nplt.plot(threshs, accs)\nplt.xlabel('Thresholds')\nplt.ylabel('Accuracy')\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nText(0, 0.5, 'Accuracy')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-13-output-2.png){width=597 height=430}\n:::\n:::\n\n\nAccording to the plot above the accuracy reaches a maximum at a threshold around 0.5, so we will us that as our cutoff. \n\n::: {#de71a853 .cell execution_count=13}\n``` {.python .cell-code}\nlr_preds = lr_probs > 0.5\n\nfpr, tpr, threshold = roc_curve(y_test, lr_probs)\n\nlr_cm = confusion_matrix(y_test, lr_preds)\n\nresultsPlot(lr_cm, fpr, tpr)\n```\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-14-output-1.png){width=767 height=449}\n:::\n:::\n\n\nThe logistic regression also did a very good job with this classification task with an accuracy score of 97.7% and an AUC of 0.995. Similarly to the KNN model the logistic regression misclassified 4 malignant samples as benign.\n\n### Decision Tree\n\nThe next model we will evaluate is a simple decision tree. The decision tree model works by using the features to split the samples into different buckets for classification. Due to the way a decision tree works feature scaling is not necessary for this model. The decision tree has the following hyperparameters that need to be tuned.\n\n- criterion: the function to measure the quality of the split at each node, options are gini impurity, log loss, and entropy\n\n- splitter: how the algorithm determines the split at each node, options are best and random\n\n- maximum depth: controls how deep the tree can get. The deeper the tree the higher likelihood of overfitting, the shallower the tree the higher the likelihood of underfitting.\n\n- minimum samples splt: the minimum number of samples needed before an internal leaf node splits\n\n- minimum samples leaf: the minimum number of samples required for a node to be a leaf node\n\n- max features: the number of samples that are considered for each leaf node. We will be using either log2(number of features) or sqrt(number of features).\n\nSimilar to the KNN model we will be using a randomized cross validation search for training and tuning the decision tree model.\n\n::: {#469426cf .cell execution_count=14}\n``` {.python .cell-code}\ndt_tune_dist = {\n    'criterion': ['gini', 'entropy', 'log_loss'],\n    'splitter': ['best', 'random'],\n    'max_depth': np.arange(1,21,1),\n    'min_samples_split': np.arange(2,21,1),\n    'min_samples_leaf': np.arange(1,21,1),\n    'max_features': ['log2', 'sqrt']\n}\n\ndt = DecisionTreeClassifier()\n\ndt_random_search = RandomizedSearchCV(\n    dt,\n    param_distributions=dt_tune_dist,\n    n_iter=100,\n    cv=5,\n    n_jobs=1,\n    random_state=2024\n)\n\ndt_random_search.fit(x_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<style>#sk-container-id-2 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-2 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-2 pre {\n  padding: 0;\n}\n\n#sk-container-id-2 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-2 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-2 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-2 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-2 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-2 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-2 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-2 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-2 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-2 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-2 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-2 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n#sk-container-id-2 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-2 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-2 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-2 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-2 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-2 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-2 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-2 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_iter=100,\n                   n_jobs=1,\n                   param_distributions={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;,\n                                                      &#x27;log_loss&#x27;],\n                                        &#x27;max_depth&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20]),\n                                        &#x27;max_features&#x27;: [&#x27;log2&#x27;, &#x27;sqrt&#x27;],\n                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20]),\n                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19, 20]),\n                                        &#x27;splitter&#x27;: [&#x27;best&#x27;, &#x27;random&#x27;]},\n                   random_state=2024)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomizedSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(cv=5, estimator=DecisionTreeClassifier(), n_iter=100,\n                   n_jobs=1,\n                   param_distributions={&#x27;criterion&#x27;: [&#x27;gini&#x27;, &#x27;entropy&#x27;,\n                                                      &#x27;log_loss&#x27;],\n                                        &#x27;max_depth&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20]),\n                                        &#x27;max_features&#x27;: [&#x27;log2&#x27;, &#x27;sqrt&#x27;],\n                                        &#x27;min_samples_leaf&#x27;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20]),\n                                        &#x27;min_samples_split&#x27;: array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19, 20]),\n                                        &#x27;splitter&#x27;: [&#x27;best&#x27;, &#x27;random&#x27;]},\n                   random_state=2024)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: DecisionTreeClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=np.int64(5), max_features=&#x27;log2&#x27;,\n                       min_samples_leaf=np.int64(12),\n                       min_samples_split=np.int64(5))</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=np.int64(5), max_features=&#x27;log2&#x27;,\n                       min_samples_leaf=np.int64(12),\n                       min_samples_split=np.int64(5))</pre></div> </div></div></div></div></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {#e52e615e .cell execution_count=15}\n``` {.python .cell-code}\ndt_best_model = dt_random_search.best_estimator_\ndt_feature_imp = dt_best_model.feature_importances_\n\ndt_feature_imp_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': dt_feature_imp\n}).sort_values('importance', ascending=False)\n\nsns.barplot(\n    dt_feature_imp_df,\n    y='feature',\n    x='importance'\n)   \n\nplt.title('Decision Tree Feature Importance')\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nText(0.5, 1.0, 'Decision Tree Feature Importance')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-16-output-2.png){width=730 height=449}\n:::\n:::\n\n\n::: {#5c00a2e5 .cell execution_count=16}\n``` {.python .cell-code}\ndt_preds = dt_random_search.predict(x_test)\ndt_probs = dt_random_search.predict_proba(x_test)[:, 1]\n\ndt_cm = confusion_matrix(y_test, dt_preds)\n\ndt_fpr, dt_tpr, thresholds = roc_curve(y_test, dt_probs)\n\nresultsPlot(dt_cm, dt_fpr, dt_tpr)\n```\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-17-output-1.png){width=767 height=449}\n:::\n:::\n\n\n### Random Forest\n\n::: {#a0cfea21 .cell execution_count=17}\n``` {.python .cell-code}\n# rf_para_dist = {\n#     'n_estimators': np.arange(1,500,1),\n#     'criterion': ['gini', 'entropy', 'log_loss'],\n#     'max_depth': np.arange(1,21,1),\n#     'min_samples_split': np.arange(2,21,1),\n#     'min_samples_leaf': np.arange(1,21,1),\n#     'max_features': ['log2', 'sqrt']\n# }\n\n# rf = RandomForestClassifier()\n\n# rf_random_search = RandomizedSearchCV(\n#     rf,\n#     param_distributions=rf_para_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# rf_random_search.fit(x_train, y_train)\n```\n:::\n\n\n::: {#86032c1a .cell execution_count=18}\n``` {.python .cell-code}\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# rf_pickle_path = ml_model_dir / 'breast_cancer_rf_model.pkl'\n# with open(rf_pickle_path, 'wb') as file:\n#     pickle.dump(rf_random_search, file)\n```\n:::\n\n\n::: {#4899da4d .cell execution_count=19}\n``` {.python .cell-code}\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_rf_model.pkl', 'rb') as file:\n    rf_random_search = pickle.load(file)\n\nrf_best_model = rf_random_search.best_estimator_\nrf_feature_imp = rf_best_model.feature_importances_\nrf_feature_imp_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': rf_feature_imp\n}).sort_values('importance', ascending=False)\n\nsns.barplot(\n    rf_feature_imp_df,\n    y='feature',\n    x='importance'\n)\nplt.title('Random Forest Feature Importance')\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\nText(0.5, 1.0, 'Random Forest Feature Importance')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-20-output-2.png){width=730 height=449}\n:::\n:::\n\n\n::: {#81d62389 .cell execution_count=20}\n``` {.python .cell-code}\nrf_preds = rf_random_search.predict(x_test)\nrf_cm = confusion_matrix(y_test, rf_preds)\nrf_probs = rf_random_search.predict_proba(x_train)[:, 1]\nrf_fpr, rf_tpr, thresholds = roc_curve(y_test, rf_preds)\n\n\nresultsPlot(rf_cm, rf_fpr, rf_tpr)\n```\n\n::: {.cell-output .cell-output-display}\n![](breast_cancer_prediction_files/figure-html/cell-21-output-1.png){width=767 height=449}\n:::\n:::\n\n\n## Comparing Models\n\n",
    "supporting": [
      "breast_cancer_prediction_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}