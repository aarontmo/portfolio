{"title":"Breast Cancer Classification","markdown":{"yaml":{"title":"Breast Cancer Classification"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n**Probelem:** Automate the diagnosis of breast cancer.\n\n**Background:** Breast cancer is one of the most prevalent forms of cancer among woment. Early detection and diagnosis (malignant or benign) is crucial for a positive response to treatment. \n\n**Goal:** The goal of the project is to train a machine learning model to predict whether breast tissue is malignant or benign. This classification will be based on several features that were extracted from microscopy images of fine needle aspirate of a breast tissue. \n\n**Data:** More information on the data can be found [here](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/data) and [here](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)\n\n```{python}\n# import libraries\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, f1_score, classification_report\n\nfrom scipy.stats import sem\n\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n```\n\n```{python}\n# read in data\ndata_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\data\")\ndata = pd.read_csv(data_dir / 'breast_cancer.csv')\n```\n\n# Exploratory Data Analysis\n\nThe data include ten morphological and texture based features of cell nuclei in each image including the mean, standard error, and largest value (mean of three largest values) for each feature. The list below shows all the features that will be used for classification.\n\n- radius \n\n- texture (standard deviation of gray-scale values)\n\n- perimeter\n\n- area\n\n- smoothness\n\n- compactness\n\n- concavity\n\n- concave points\n\n- symmetry\n\n- fractal dimension\n\nWe can get a good idea of what features will be important in the classification model by looking at the distributions of each feature individually subsetted by the diagnosis. Because there are 30 feature columns we will only look at the mean of each the features above.\n\n```{python}\n# subset data for features ending in _mean and diagnosis\nmean_df = (\n    data\n    .filter(regex='mean|diagnosis')\n)\n\nmean_cols = [col for col in mean_df.columns if 'mean' in col]\n\nfor col in mean_cols:\n    plt.figure(figsize=(9.5,6))\n\n    sns.histplot(\n        mean_df,\n        x=col,\n        hue='diagnosis',\n        kde=True,\n        stat='density'\n    )\n    \n    plt.title(f'Distribution of {col}')\n    plt.show()\n```\n\nA few observations based on these plots:\n\n- It appears (with the a few exceptions) that the malignant group generally has a broader distribution comapared to the benign group which appears to have a tighter spread (more on this later).\n\n- With the exception of fractal dimension, symmetry, and smoothness there appears to be good separation between the malignant and benign distribtusions\n\n- The malignant group appears to have higher values on average compared to the benign group\n\nI am curious about the spread of the malignant and benign groups. Below is a table showing the standard deviations of the ten features.\n\n```{python}\n(\n    data\n    .filter(regex='mean|diagnosis')\n    .groupby('diagnosis')\n    .agg('std')\n)\n```\n\nFor most of the features the standard deviation of the malginant group is larger than benign group. The features where this is most apparent are the morphological features such as radius (3.20 vs 1.78) and perimeter (21.85 vs 11.81). It is generally known that cancer cells have irregular shapes and sizes some being larger and some being smaller than normal cells. This is a reasonable explanation for the wider spread of morphological features in the malignant group.\n\nOne of biggest concerns with classification tasks is how balanced the outcome variable is. The barchart below shows the total number of malignant and benign breat tissue samples.\n\n```{python}\n# diagnosis counts\nax = sns.countplot(\n    data,\n    x='diagnosis'\n)\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        p.get_x() + p.get_width() / 2,\n        height + 3,\n        f'{height:.0f}',\n        ha=\"center\",\n    )\n\nplt.title('Outcome Count')\nplt.xlabel('Diagnosis')\nplt.show()\n\n```\n\n63% of the observations are benign. The outcome is slightly unbalanced but no to the point where we would need to use any imputation  or oversampling method.\n\n# Predictive Modeling\n\nThe following five machine learning algorithms will be trained and evaluated to verify which is the most accurate for this problem:\n\n- K-Nearest Neighbors (KNN)\n\n- Logistic Regression\n\n- Decision Tree Classifier\n\n- Random Forest Classifier\n\n## Data Preprocessing\n\nThree preprocessing steps are necessary before training any machine learning algorithms: feature scaling, outcome binarization, and train test split. While feature scaling is necessary for some machine learning algorithms it is less imortant for others. Machine learning algorithms that rely on measuring distances between data points and boundaries require feature scaling to ensure each feature contributes equally to the classification. KNN, logistic regression, and support vector classifier require feature scaling while decision tree and random forest classifiers do not because they are non-parametric machine learning models.\n\n**Outcome Binarization**\n\nMost machine learning algorithms in the Scikit-Learn library assume the outcome to be binary so we will dichotomize the diagnosis as follows:\n\n- malignant = 1\n\n- benign = 0\n\n**Splitting Data**\n\nBefore splitting the data we want to check if there are any missing values in any of the columns\n\n```{python}\ndata.isnull().sum()\n```\n\nIt appears there are no missing values in any features except for the last column `Unnamed: 32` which are all empty. Next we will remove the empty column and the id column since it is not relevant to predicting diagnosis outcome.\n\n```{python}\nclean_data = (\n    data\n    .drop(columns=['id', 'Unnamed: 32'], axis=1)\n    .assign(diagnosis = np.where(data['diagnosis'] == 'M', 1, 0))\n)\n```\n\nWe will use a 70:30 training:test split which will result in 398 training samples and 171 testing samples. We will also stratify by diagnosis to ensure we the same proportions of malignant to benign samples in the training and testing set.\n\n```{python}\n# split into features and labels\nX = clean_data.drop(columns=['diagnosis'], axis=1)\ny = clean_data[['diagnosis']]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=2024)\n\n# ensure class proportions was preserver between training and testing splits\nfig, ax = plt.subplots(1,2)\n\nsns.countplot(\n    y_train,\n    x='diagnosis',\n    ax=ax[0]\n    )\n\nax[0].set_title('Count of Training Labels')\n\nsns.countplot(\n    y_test,\n    x='diagnosis',\n    ax=ax[1]\n    )\n\nax[1].set_title('Count of Testing Labels')\n```\n\nThe plot above verifies that the proportion of positive and negative labels was preserved between the training and testing set.\n\nWe will perform the scaling before the training of the algorithms where it is required.\n\n## Model Training\n\n### K-Nearest Neighbors\n\nThe first model we will use is the K-Nearest neighbors model. This model works by assigning a label to a value based on its K nearest neighbors. The distance between points must be calculated using some distance measure. Below is an explanation of each hyperparameter. Because this model relys on distances between points we will scale the data before training.\n\n**Hyperparameters**\n\n- n_neighbors: The number of nearest neighbors to consider when making a prediction. Larger values lead to a more generalized prediction with the risk of underfitting the data. Small values lead to a model that is more sensitive to noise in the data potentially leading to overfitting.\n\n- weights: Determines how the distance between the point of interest and its nearest neighbors influence the prediction. Uniform weights consider all neighbors equally when getting a prediction. With distance weights closer neighbors have a larger impact on the prediction.\n\n- algorithm: The algorithm that is used to compute the neighbors: brute force, ball tree, KD tree, auto\n\n- leaf_size: Parameter passed to BallTree or KDTree algorithm, can effect the speed of the training\n\n- p: power parameter passed to the Minkowski metric. When p = 1 distance metric is city block, when p = 2 distance metric is euclidean. \n\nWe will be using a randomized search with a 5 fold cross validation to train the model and tune the hyperparameters.\n\n```{python}\n#| echo: true\n\nscaler = StandardScaler()\n\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\ny_train = np.array(y_train).ravel()\ny_test = np.array(y_test).ravel()\n\n# knn_param_dist = {\n#     'n_neighbors': np.arange(1,31),\n#     'weights': ['uniform', 'distance'],\n#     'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto'],\n#     'leaf_size': np.arange(10, 51, 5),\n#     'p': [1,2]\n# }\n\n# knn = KNeighborsClassifier()\n\n# knn_random_search = RandomizedSearchCV(\n#     knn, \n#     param_distributions=knn_param_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# knn_random_search.fit(x_train_scaled, y_train)\n```\n\n```{python}\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# knn_pickle_path = ml_model_dir / 'breast_cancer_knn_model.pkl'\n# with open(knn_pickle_path, 'wb') as file:\n#     pickle.dump(knn_random_search, file)\n```\n\nThe table below shows the hyperparameters that were chosen to produce the best result.\n\n```{python}\n\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_knn_model.pkl', 'rb') as file:\n    knn_random_search = pickle.load(file)\n\npd.DataFrame([knn_random_search.best_params_])\n```\n\n```{python}\nknn_preds = knn_random_search.predict(x_test_scaled)\nknn_probs = knn_random_search.predict_proba(x_test_scaled)[:, 1]\n\naccuracy = accuracy_score(y_test, knn_preds)\ncm = confusion_matrix(y_test, knn_preds)\n\nfpr, tpr, thresholds = roc_curve(y_test, knn_probs)\n\ntick_labels = data[['diagnosis']].drop_duplicates()\n\ndef resultsPlot(confusion_matrix, fpr, tpr):\n\n    roc_auc = auc(fpr, tpr)\n\n    fig, ax = plt.subplots(1,2, figsize=(9.5,5))\n    sns.heatmap(\n        confusion_matrix, \n        annot=True, \n        fmt='d', \n        cmap='Blues', \n        xticklabels=['B', 'M'], \n        yticklabels=['B', 'M'],\n        ax=ax[0])\n    ax[0].set_title('Confusion Matrix')\n    ax[0].set_xlabel('Predicted Label')\n    ax[0].set_ylabel('True Label')\n\n    ax[1].plot(\n        fpr,\n        tpr,\n        color='darkorange',\n        lw=2,\n        label=f'AUC = {roc_auc:.2f}'\n    )\n    ax[1].plot(\n        [0,1], \n        [0,1],\n        color='navy',\n        lw=2,\n        linestyle='--'\n    )\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')\n    ax[1].set_title('ROC Curve')\n    ax[1].legend(loc='lower right')\n\nresultsPlot(cm, fpr, tpr)\n```\n\n```{python}\npd.DataFrame(\n    classification_report(y_test, knn_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n```\n\nAs shown by the validation results above, this model performed very well with an f1-score of 0.95 and AUC score of 0.997. As you can see from the confusion matrix there were 6 false negatives where the model incorrectly predicted a sample as benign when it was malignant.\n\n### Logistic Regression\n\nThe next model we will use is the logistic regression. Traditionally the logistic regression returns odds of a value being positive. However we can use the logistic regression as a classifier by converting the odds into probabilities and then setting a threshold to classify the samples into two categories. Because this project is primarily concerned with prediction I won't discuss the mathematical model and the effects of different beta coefficients. \n\n```{python}\nlr = LogisticRegression(fit_intercept=True)\n\nlr.fit(x_train_scaled, y_train)\n\nlr_probs = lr.predict_proba(x_test_scaled)[:,1]\nthreshs = np.arange(0.1,1,0.01)\n\naccs = []\nfor thresh in threshs:\n    preds = lr_probs > thresh\n    acc = accuracy_score(y_test, preds)\n    accs.append(acc)\n\nacc_df = pd.DataFrame(\n    {\n        'threshs': threshs,\n        'accs': accs})\n\nplt.plot(threshs, accs)\nplt.xlabel('Thresholds')\nplt.ylabel('Accuracy')\n```\n\nAccording to the plot above the accuracy reaches a maximum at a threshold around 0.5, so we will us that as our cutoff. \n\n```{python}\nlr_preds = lr_probs > 0.5\n\nfpr, tpr, threshold = roc_curve(y_test, lr_probs)\n\nlr_cm = confusion_matrix(y_test, lr_preds)\n\nresultsPlot(lr_cm, fpr, tpr)\n\n```\n\n```{python}\npd.DataFrame(\n    classification_report(y_test, lr_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n```\n\nThe logistic regression also did a very good job with this classification task with an f1-scor of 0.97 and an AUC of 0.995. Similarly to the KNN model the logistic regression misclassified 4 malignant samples as benign.\n\n### Decision Tree\n\nThe next model we will evaluate is a simple decision tree. The decision tree model works by using the features to split the samples into different buckets for classification. Due to the way a decision tree works feature scaling is not necessary for this model. The decision tree has the following hyperparameters that need to be tuned.\n\n- criterion: the function to measure the quality of the split at each node, options are gini impurity, log loss, and entropy\n\n- splitter: how the algorithm determines the split at each node, options are best and random\n\n- maximum depth: controls how deep the tree can get. The deeper the tree the higher likelihood of overfitting, the shallower the tree the higher the likelihood of underfitting.\n\n- minimum samples splt: the minimum number of samples needed before an internal leaf node splits\n\n- minimum samples leaf: the minimum number of samples required for a node to be a leaf node\n\n- max features: the number of samples that are considered for each leaf node. We will be using either log2(number of features) or sqrt(number of features).\n\nSimilar to the KNN model we will be using a randomized cross validation search for training and tuning the decision tree model.\n\n```{python}\n# dt_tune_dist = {\n#     'criterion': ['gini', 'entropy', 'log_loss'],\n#     'splitter': ['best', 'random'],\n#     'max_depth': np.arange(1,21,1),\n#     'min_samples_split': np.arange(2,21,1),\n#     'min_samples_leaf': np.arange(1,21,1),\n#     'max_features': ['log2', 'sqrt']\n# }\n\n# dt = DecisionTreeClassifier()\n\n# dt_random_search = RandomizedSearchCV(\n#     dt,\n#     param_distributions=dt_tune_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# dt_random_search.fit(x_train, y_train)\n\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# dt_pickle_path = ml_model_dir / 'breast_cancer_dt_model.pkl'\n# with open(dt_pickle_path, 'wb') as file:\n#     pickle.dump(dt_random_search, file)\n\n```\n\n```{python}\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_dt_model.pkl', 'rb') as file:\n    dt_random_search = pickle.load(file)\n\n\ndt_best_model = dt_random_search.best_estimator_\ndt_feature_imp = dt_best_model.feature_importances_\n\ndt_feature_imp_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': dt_feature_imp\n}).sort_values('importance', ascending=False)\n\nsns.barplot(\n    dt_feature_imp_df,\n    y='feature',\n    x='importance'\n)   \nplt.title('Decision Tree Feature Importance')\n\n\n```\n\nThe bar chart above shows the feature importances of the features that were used in the decision tree. The worst perimeter was the main feature that was used to make the prediction. The worst perimeter is defined at the mean of the three largest cell perimeters.\n\n```{python}\n\nplot_tree(\n    dt_best_model,\n    filled=False,\n    feature_names=x_train.columns,\n    rounded=True\n)\nimg_dir = Path(os.getcwd() + '/data/images')\nimg_file = 'breast_cancer_dt.png'\nplt.savefig(img_dir / img_file)\nplt.close()\n\n```\n\nThe image below shows the actual decision tree that was used to make the predictions. Each node shows the feature that was used and the cutoff for that feature. This aids in the understanding of how decision trees work.\n\n![Decision Tree Plot](data/images/breast_cancer_dt.png){.lightbox}\n\n```{python}\ndt_preds = dt_random_search.predict(x_test)\ndt_probs = dt_random_search.predict_proba(x_test)[:, 1]\n\ndt_cm = confusion_matrix(y_test, dt_preds)\n\ndt_fpr, dt_tpr, thresholds = roc_curve(y_test, dt_probs)\n\nresultsPlot(dt_cm, dt_fpr, dt_tpr)\n\n```\n\n```{python}\npd.DataFrame(\n    classification_report(y_test, dt_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n```\n\nBecause of the simplicity of the decision tree model it tends not to perform quite as well as other more complex models. The model returned an f1-score of 0.84 and an AUC score of 0.93. The model misclassified 12 malignant samples as benign (false negative) and 8 benign samples as malignant (false positive).\n\n### Random Forest\n\nThe random forest model is an extension of the decision tree model. The randome forest model fits several smaller decision trees and outputting the mode prediction for each sample. There several pros and cons to decision trees; the main pro is that is more accurate that a single decision tree because it combines several weak predictors to make a more accurate prediction. One of the main cons of a random forest classifier is that the increased complexity increases the training time and the ability to interpret the model. The hyperparameters to be tuned are shown below:\n\n- n_estimators: how many individual decision trees to train\n\n- criterion: the function to measure the quality of the split at each node, options are gini impurity, log loss, and entropy\n\n- maximum depth: controls how deep the tree can get. The deeper the tree the higher likelihood of overfitting, the shallower the tree the higher the likelihood of underfitting.\n\n- minimum samples splt: the minimum number of samples needed before an internal leaf node splits\n\n- minimum samples leaf: the minimum number of samples required for a node to be a leaf node\n\n- max features: the number of samples that are considered for each leaf node. We will be using either log2(number of features) or sqrt(number of features).\n\n```{python}\n\n# rf_para_dist = {\n#     'n_estimators': np.arange(1,500,1),\n#     'criterion': ['gini', 'entropy', 'log_loss'],\n#     'max_depth': np.arange(1,21,1),\n#     'min_samples_split': np.arange(2,21,1),\n#     'min_samples_leaf': np.arange(1,21,1),\n#     'max_features': ['log2', 'sqrt']\n# }\n\n# rf = RandomForestClassifier()\n\n# rf_random_search = RandomizedSearchCV(\n#     rf,\n#     param_distributions=rf_para_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# rf_random_search.fit(x_train, y_train)\n\n```\n\n```{python}\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# rf_pickle_path = ml_model_dir / 'breast_cancer_rf_model.pkl'\n# with open(rf_pickle_path, 'wb') as file:\n#     pickle.dump(rf_random_search, file)\n```\n\n```{python}\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_rf_model.pkl', 'rb') as file:\n    rf_random_search = pickle.load(file)\n\nrf_best_model = rf_random_search.best_estimator_\nrf_feature_imp = rf_best_model.feature_importances_\nrf_feature_imp_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': rf_feature_imp\n}).sort_values('importance', ascending=False)\n\nsns.barplot(\n    rf_feature_imp_df,\n    y='feature',\n    x='importance'\n)\nplt.title('Random Forest Feature Importance')\n\n```\n\nThe plot above shows the feature importances for all the features that were used in the predictions. An obvious difference between this model and simple decision tree is that the random forest model uses all the features.\n\n```{python}\n\nrf_preds = rf_random_search.predict(x_test)\nrf_cm = confusion_matrix(y_test, rf_preds)\nrf_probs = rf_random_search.predict_proba(x_train)[:, 1]\nrf_fpr, rf_tpr, thresholds = roc_curve(y_test, rf_preds)\n\n\nresultsPlot(rf_cm, rf_fpr, rf_tpr)\n```\n\n```{python}\npd.DataFrame(\n    classification_report(y_test, rf_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n```\n\nThe random forest model returned an f1-score of 0.93 and an AUC score 0.94. There were six false negatives and 3 false positives. The following section compares the accuracy of all the models to decide on the best model.\n\n# Conclusion\n\nWe are able to easily compare models with the plot below. The plot below shows the mean accuracy of all iterations during the hyperparameter and training step of fitting the models along with the 95% confidence interval. Because we didn't use a randomized cross validation method to find the hyperparameters for the logistic regression model it is not included in the plot below.\n\n```{python}\nmodels = {\n    'knn': knn_random_search,\n    'dt': dt_random_search,\n    'rf': rf_random_search\n}\ndfs = []\nfor name, model in models.items():\n    this_dict = {\n        f'mean_acc': model.cv_results_['mean_test_score']\n    }\n\n    this_df = pd.DataFrame(this_dict)\n    dfs.append(this_df)\n    \nmodel_res = pd.concat(dfs)\nnested_col = [['knn']*100, ['dt']*100, ['rf']*100]\nmodel_col = [item for sublist in nested_col for item in sublist]\nmodel_res['model'] = model_col\n\n\nfinal_df = (\n    model_res\n    .groupby('model')\n    .agg(\n        mean_val=('mean_acc', 'mean'),\n        conf_int=('mean_acc', lambda x: sem(x)*1.96))\n    .reset_index()\n)\n\nsns.scatterplot(\n    final_df,\n    y='model',\n    x='mean_val'\n)\n\nfor i in range(final_df.shape[0]):\n    plt.errorbar(x=final_df['mean_val'][i], \n                 y=final_df['model'][i],\n                 xerr=final_df['conf_int'][i], \n                 fmt='o', color='black', capsize=5)\n\nplt.title('Mean Model Accuracy with Confidence Interval')\nplt.xlabel('Accuracy')\nplt.ylabel('')\nplt.yticks(range(3), labels=['Decision Tree', 'K-Nearest Neighbors', 'Random Forest'])\n\nplt.show()\n\n```\n\nAs mentioned earlier all the machine learing models performed very well with this prediction task. Admittedly this dataset is very clean and very little transformations or feature processing was needed to achieve a high model accuracy. \n\nWhen choosing a final model for prediction there are several factors that need to be considered: model accuracy, model simplicity, interpretability, and false positive vs false negative trade off. The last factor is particularly interesting because it requires some domain knowledge of the particular task at hand. Do we care more about catching everybody that has malignat breast cancer at the expense of falsely diagnosing those with benign cancer as malignant? Or do we care more about ensuring that we identify all those that are benign at the expense of misdiagnosing those with malignant breast cancer. \n\nReturning to goal of predictions like this, to diagnose breast cancer early on in its course, I believe it is more beneficial to choose a model that has a high sensitivity. This ensures that we catch as many true positive cases early on as a preliminary diagnoses. In our case the logistic regression model had the highest sensitivity (0.94) of all the models we tested. The logistic regression also has the advantage of being highly interpretable which we didn't discuss in this project because we were only concerned with predictability. My susggestion for this data would be to use a logistic regression model to predict breast cancer type.\n","srcMarkdownNoYaml":"\n\n# Introduction\n\n**Probelem:** Automate the diagnosis of breast cancer.\n\n**Background:** Breast cancer is one of the most prevalent forms of cancer among woment. Early detection and diagnosis (malignant or benign) is crucial for a positive response to treatment. \n\n**Goal:** The goal of the project is to train a machine learning model to predict whether breast tissue is malignant or benign. This classification will be based on several features that were extracted from microscopy images of fine needle aspirate of a breast tissue. \n\n**Data:** More information on the data can be found [here](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data/data) and [here](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)\n\n```{python}\n# import libraries\nimport os\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, f1_score, classification_report\n\nfrom scipy.stats import sem\n\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n```\n\n```{python}\n# read in data\ndata_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\data\")\ndata = pd.read_csv(data_dir / 'breast_cancer.csv')\n```\n\n# Exploratory Data Analysis\n\nThe data include ten morphological and texture based features of cell nuclei in each image including the mean, standard error, and largest value (mean of three largest values) for each feature. The list below shows all the features that will be used for classification.\n\n- radius \n\n- texture (standard deviation of gray-scale values)\n\n- perimeter\n\n- area\n\n- smoothness\n\n- compactness\n\n- concavity\n\n- concave points\n\n- symmetry\n\n- fractal dimension\n\nWe can get a good idea of what features will be important in the classification model by looking at the distributions of each feature individually subsetted by the diagnosis. Because there are 30 feature columns we will only look at the mean of each the features above.\n\n```{python}\n# subset data for features ending in _mean and diagnosis\nmean_df = (\n    data\n    .filter(regex='mean|diagnosis')\n)\n\nmean_cols = [col for col in mean_df.columns if 'mean' in col]\n\nfor col in mean_cols:\n    plt.figure(figsize=(9.5,6))\n\n    sns.histplot(\n        mean_df,\n        x=col,\n        hue='diagnosis',\n        kde=True,\n        stat='density'\n    )\n    \n    plt.title(f'Distribution of {col}')\n    plt.show()\n```\n\nA few observations based on these plots:\n\n- It appears (with the a few exceptions) that the malignant group generally has a broader distribution comapared to the benign group which appears to have a tighter spread (more on this later).\n\n- With the exception of fractal dimension, symmetry, and smoothness there appears to be good separation between the malignant and benign distribtusions\n\n- The malignant group appears to have higher values on average compared to the benign group\n\nI am curious about the spread of the malignant and benign groups. Below is a table showing the standard deviations of the ten features.\n\n```{python}\n(\n    data\n    .filter(regex='mean|diagnosis')\n    .groupby('diagnosis')\n    .agg('std')\n)\n```\n\nFor most of the features the standard deviation of the malginant group is larger than benign group. The features where this is most apparent are the morphological features such as radius (3.20 vs 1.78) and perimeter (21.85 vs 11.81). It is generally known that cancer cells have irregular shapes and sizes some being larger and some being smaller than normal cells. This is a reasonable explanation for the wider spread of morphological features in the malignant group.\n\nOne of biggest concerns with classification tasks is how balanced the outcome variable is. The barchart below shows the total number of malignant and benign breat tissue samples.\n\n```{python}\n# diagnosis counts\nax = sns.countplot(\n    data,\n    x='diagnosis'\n)\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        p.get_x() + p.get_width() / 2,\n        height + 3,\n        f'{height:.0f}',\n        ha=\"center\",\n    )\n\nplt.title('Outcome Count')\nplt.xlabel('Diagnosis')\nplt.show()\n\n```\n\n63% of the observations are benign. The outcome is slightly unbalanced but no to the point where we would need to use any imputation  or oversampling method.\n\n# Predictive Modeling\n\nThe following five machine learning algorithms will be trained and evaluated to verify which is the most accurate for this problem:\n\n- K-Nearest Neighbors (KNN)\n\n- Logistic Regression\n\n- Decision Tree Classifier\n\n- Random Forest Classifier\n\n## Data Preprocessing\n\nThree preprocessing steps are necessary before training any machine learning algorithms: feature scaling, outcome binarization, and train test split. While feature scaling is necessary for some machine learning algorithms it is less imortant for others. Machine learning algorithms that rely on measuring distances between data points and boundaries require feature scaling to ensure each feature contributes equally to the classification. KNN, logistic regression, and support vector classifier require feature scaling while decision tree and random forest classifiers do not because they are non-parametric machine learning models.\n\n**Outcome Binarization**\n\nMost machine learning algorithms in the Scikit-Learn library assume the outcome to be binary so we will dichotomize the diagnosis as follows:\n\n- malignant = 1\n\n- benign = 0\n\n**Splitting Data**\n\nBefore splitting the data we want to check if there are any missing values in any of the columns\n\n```{python}\ndata.isnull().sum()\n```\n\nIt appears there are no missing values in any features except for the last column `Unnamed: 32` which are all empty. Next we will remove the empty column and the id column since it is not relevant to predicting diagnosis outcome.\n\n```{python}\nclean_data = (\n    data\n    .drop(columns=['id', 'Unnamed: 32'], axis=1)\n    .assign(diagnosis = np.where(data['diagnosis'] == 'M', 1, 0))\n)\n```\n\nWe will use a 70:30 training:test split which will result in 398 training samples and 171 testing samples. We will also stratify by diagnosis to ensure we the same proportions of malignant to benign samples in the training and testing set.\n\n```{python}\n# split into features and labels\nX = clean_data.drop(columns=['diagnosis'], axis=1)\ny = clean_data[['diagnosis']]\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=2024)\n\n# ensure class proportions was preserver between training and testing splits\nfig, ax = plt.subplots(1,2)\n\nsns.countplot(\n    y_train,\n    x='diagnosis',\n    ax=ax[0]\n    )\n\nax[0].set_title('Count of Training Labels')\n\nsns.countplot(\n    y_test,\n    x='diagnosis',\n    ax=ax[1]\n    )\n\nax[1].set_title('Count of Testing Labels')\n```\n\nThe plot above verifies that the proportion of positive and negative labels was preserved between the training and testing set.\n\nWe will perform the scaling before the training of the algorithms where it is required.\n\n## Model Training\n\n### K-Nearest Neighbors\n\nThe first model we will use is the K-Nearest neighbors model. This model works by assigning a label to a value based on its K nearest neighbors. The distance between points must be calculated using some distance measure. Below is an explanation of each hyperparameter. Because this model relys on distances between points we will scale the data before training.\n\n**Hyperparameters**\n\n- n_neighbors: The number of nearest neighbors to consider when making a prediction. Larger values lead to a more generalized prediction with the risk of underfitting the data. Small values lead to a model that is more sensitive to noise in the data potentially leading to overfitting.\n\n- weights: Determines how the distance between the point of interest and its nearest neighbors influence the prediction. Uniform weights consider all neighbors equally when getting a prediction. With distance weights closer neighbors have a larger impact on the prediction.\n\n- algorithm: The algorithm that is used to compute the neighbors: brute force, ball tree, KD tree, auto\n\n- leaf_size: Parameter passed to BallTree or KDTree algorithm, can effect the speed of the training\n\n- p: power parameter passed to the Minkowski metric. When p = 1 distance metric is city block, when p = 2 distance metric is euclidean. \n\nWe will be using a randomized search with a 5 fold cross validation to train the model and tune the hyperparameters.\n\n```{python}\n#| echo: true\n\nscaler = StandardScaler()\n\nx_train_scaled = scaler.fit_transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\ny_train = np.array(y_train).ravel()\ny_test = np.array(y_test).ravel()\n\n# knn_param_dist = {\n#     'n_neighbors': np.arange(1,31),\n#     'weights': ['uniform', 'distance'],\n#     'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto'],\n#     'leaf_size': np.arange(10, 51, 5),\n#     'p': [1,2]\n# }\n\n# knn = KNeighborsClassifier()\n\n# knn_random_search = RandomizedSearchCV(\n#     knn, \n#     param_distributions=knn_param_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# knn_random_search.fit(x_train_scaled, y_train)\n```\n\n```{python}\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# knn_pickle_path = ml_model_dir / 'breast_cancer_knn_model.pkl'\n# with open(knn_pickle_path, 'wb') as file:\n#     pickle.dump(knn_random_search, file)\n```\n\nThe table below shows the hyperparameters that were chosen to produce the best result.\n\n```{python}\n\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_knn_model.pkl', 'rb') as file:\n    knn_random_search = pickle.load(file)\n\npd.DataFrame([knn_random_search.best_params_])\n```\n\n```{python}\nknn_preds = knn_random_search.predict(x_test_scaled)\nknn_probs = knn_random_search.predict_proba(x_test_scaled)[:, 1]\n\naccuracy = accuracy_score(y_test, knn_preds)\ncm = confusion_matrix(y_test, knn_preds)\n\nfpr, tpr, thresholds = roc_curve(y_test, knn_probs)\n\ntick_labels = data[['diagnosis']].drop_duplicates()\n\ndef resultsPlot(confusion_matrix, fpr, tpr):\n\n    roc_auc = auc(fpr, tpr)\n\n    fig, ax = plt.subplots(1,2, figsize=(9.5,5))\n    sns.heatmap(\n        confusion_matrix, \n        annot=True, \n        fmt='d', \n        cmap='Blues', \n        xticklabels=['B', 'M'], \n        yticklabels=['B', 'M'],\n        ax=ax[0])\n    ax[0].set_title('Confusion Matrix')\n    ax[0].set_xlabel('Predicted Label')\n    ax[0].set_ylabel('True Label')\n\n    ax[1].plot(\n        fpr,\n        tpr,\n        color='darkorange',\n        lw=2,\n        label=f'AUC = {roc_auc:.2f}'\n    )\n    ax[1].plot(\n        [0,1], \n        [0,1],\n        color='navy',\n        lw=2,\n        linestyle='--'\n    )\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')\n    ax[1].set_title('ROC Curve')\n    ax[1].legend(loc='lower right')\n\nresultsPlot(cm, fpr, tpr)\n```\n\n```{python}\npd.DataFrame(\n    classification_report(y_test, knn_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n```\n\nAs shown by the validation results above, this model performed very well with an f1-score of 0.95 and AUC score of 0.997. As you can see from the confusion matrix there were 6 false negatives where the model incorrectly predicted a sample as benign when it was malignant.\n\n### Logistic Regression\n\nThe next model we will use is the logistic regression. Traditionally the logistic regression returns odds of a value being positive. However we can use the logistic regression as a classifier by converting the odds into probabilities and then setting a threshold to classify the samples into two categories. Because this project is primarily concerned with prediction I won't discuss the mathematical model and the effects of different beta coefficients. \n\n```{python}\nlr = LogisticRegression(fit_intercept=True)\n\nlr.fit(x_train_scaled, y_train)\n\nlr_probs = lr.predict_proba(x_test_scaled)[:,1]\nthreshs = np.arange(0.1,1,0.01)\n\naccs = []\nfor thresh in threshs:\n    preds = lr_probs > thresh\n    acc = accuracy_score(y_test, preds)\n    accs.append(acc)\n\nacc_df = pd.DataFrame(\n    {\n        'threshs': threshs,\n        'accs': accs})\n\nplt.plot(threshs, accs)\nplt.xlabel('Thresholds')\nplt.ylabel('Accuracy')\n```\n\nAccording to the plot above the accuracy reaches a maximum at a threshold around 0.5, so we will us that as our cutoff. \n\n```{python}\nlr_preds = lr_probs > 0.5\n\nfpr, tpr, threshold = roc_curve(y_test, lr_probs)\n\nlr_cm = confusion_matrix(y_test, lr_preds)\n\nresultsPlot(lr_cm, fpr, tpr)\n\n```\n\n```{python}\npd.DataFrame(\n    classification_report(y_test, lr_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n```\n\nThe logistic regression also did a very good job with this classification task with an f1-scor of 0.97 and an AUC of 0.995. Similarly to the KNN model the logistic regression misclassified 4 malignant samples as benign.\n\n### Decision Tree\n\nThe next model we will evaluate is a simple decision tree. The decision tree model works by using the features to split the samples into different buckets for classification. Due to the way a decision tree works feature scaling is not necessary for this model. The decision tree has the following hyperparameters that need to be tuned.\n\n- criterion: the function to measure the quality of the split at each node, options are gini impurity, log loss, and entropy\n\n- splitter: how the algorithm determines the split at each node, options are best and random\n\n- maximum depth: controls how deep the tree can get. The deeper the tree the higher likelihood of overfitting, the shallower the tree the higher the likelihood of underfitting.\n\n- minimum samples splt: the minimum number of samples needed before an internal leaf node splits\n\n- minimum samples leaf: the minimum number of samples required for a node to be a leaf node\n\n- max features: the number of samples that are considered for each leaf node. We will be using either log2(number of features) or sqrt(number of features).\n\nSimilar to the KNN model we will be using a randomized cross validation search for training and tuning the decision tree model.\n\n```{python}\n# dt_tune_dist = {\n#     'criterion': ['gini', 'entropy', 'log_loss'],\n#     'splitter': ['best', 'random'],\n#     'max_depth': np.arange(1,21,1),\n#     'min_samples_split': np.arange(2,21,1),\n#     'min_samples_leaf': np.arange(1,21,1),\n#     'max_features': ['log2', 'sqrt']\n# }\n\n# dt = DecisionTreeClassifier()\n\n# dt_random_search = RandomizedSearchCV(\n#     dt,\n#     param_distributions=dt_tune_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# dt_random_search.fit(x_train, y_train)\n\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# dt_pickle_path = ml_model_dir / 'breast_cancer_dt_model.pkl'\n# with open(dt_pickle_path, 'wb') as file:\n#     pickle.dump(dt_random_search, file)\n\n```\n\n```{python}\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_dt_model.pkl', 'rb') as file:\n    dt_random_search = pickle.load(file)\n\n\ndt_best_model = dt_random_search.best_estimator_\ndt_feature_imp = dt_best_model.feature_importances_\n\ndt_feature_imp_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': dt_feature_imp\n}).sort_values('importance', ascending=False)\n\nsns.barplot(\n    dt_feature_imp_df,\n    y='feature',\n    x='importance'\n)   \nplt.title('Decision Tree Feature Importance')\n\n\n```\n\nThe bar chart above shows the feature importances of the features that were used in the decision tree. The worst perimeter was the main feature that was used to make the prediction. The worst perimeter is defined at the mean of the three largest cell perimeters.\n\n```{python}\n\nplot_tree(\n    dt_best_model,\n    filled=False,\n    feature_names=x_train.columns,\n    rounded=True\n)\nimg_dir = Path(os.getcwd() + '/data/images')\nimg_file = 'breast_cancer_dt.png'\nplt.savefig(img_dir / img_file)\nplt.close()\n\n```\n\nThe image below shows the actual decision tree that was used to make the predictions. Each node shows the feature that was used and the cutoff for that feature. This aids in the understanding of how decision trees work.\n\n![Decision Tree Plot](data/images/breast_cancer_dt.png){.lightbox}\n\n```{python}\ndt_preds = dt_random_search.predict(x_test)\ndt_probs = dt_random_search.predict_proba(x_test)[:, 1]\n\ndt_cm = confusion_matrix(y_test, dt_preds)\n\ndt_fpr, dt_tpr, thresholds = roc_curve(y_test, dt_probs)\n\nresultsPlot(dt_cm, dt_fpr, dt_tpr)\n\n```\n\n```{python}\npd.DataFrame(\n    classification_report(y_test, dt_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n```\n\nBecause of the simplicity of the decision tree model it tends not to perform quite as well as other more complex models. The model returned an f1-score of 0.84 and an AUC score of 0.93. The model misclassified 12 malignant samples as benign (false negative) and 8 benign samples as malignant (false positive).\n\n### Random Forest\n\nThe random forest model is an extension of the decision tree model. The randome forest model fits several smaller decision trees and outputting the mode prediction for each sample. There several pros and cons to decision trees; the main pro is that is more accurate that a single decision tree because it combines several weak predictors to make a more accurate prediction. One of the main cons of a random forest classifier is that the increased complexity increases the training time and the ability to interpret the model. The hyperparameters to be tuned are shown below:\n\n- n_estimators: how many individual decision trees to train\n\n- criterion: the function to measure the quality of the split at each node, options are gini impurity, log loss, and entropy\n\n- maximum depth: controls how deep the tree can get. The deeper the tree the higher likelihood of overfitting, the shallower the tree the higher the likelihood of underfitting.\n\n- minimum samples splt: the minimum number of samples needed before an internal leaf node splits\n\n- minimum samples leaf: the minimum number of samples required for a node to be a leaf node\n\n- max features: the number of samples that are considered for each leaf node. We will be using either log2(number of features) or sqrt(number of features).\n\n```{python}\n\n# rf_para_dist = {\n#     'n_estimators': np.arange(1,500,1),\n#     'criterion': ['gini', 'entropy', 'log_loss'],\n#     'max_depth': np.arange(1,21,1),\n#     'min_samples_split': np.arange(2,21,1),\n#     'min_samples_leaf': np.arange(1,21,1),\n#     'max_features': ['log2', 'sqrt']\n# }\n\n# rf = RandomForestClassifier()\n\n# rf_random_search = RandomizedSearchCV(\n#     rf,\n#     param_distributions=rf_para_dist,\n#     n_iter=100,\n#     cv=5,\n#     n_jobs=1,\n#     random_state=2024\n# )\n\n# rf_random_search.fit(x_train, y_train)\n\n```\n\n```{python}\n# ml_model_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\n# rf_pickle_path = ml_model_dir / 'breast_cancer_rf_model.pkl'\n# with open(rf_pickle_path, 'wb') as file:\n#     pickle.dump(rf_random_search, file)\n```\n\n```{python}\nml_models_dir = Path(r\"C:\\Users\\aaron\\Desktop\\projects\\breast_cancer_prediction\\ml_models\")\nwith open(ml_models_dir / 'breast_cancer_rf_model.pkl', 'rb') as file:\n    rf_random_search = pickle.load(file)\n\nrf_best_model = rf_random_search.best_estimator_\nrf_feature_imp = rf_best_model.feature_importances_\nrf_feature_imp_df = pd.DataFrame({\n    'feature': x_train.columns,\n    'importance': rf_feature_imp\n}).sort_values('importance', ascending=False)\n\nsns.barplot(\n    rf_feature_imp_df,\n    y='feature',\n    x='importance'\n)\nplt.title('Random Forest Feature Importance')\n\n```\n\nThe plot above shows the feature importances for all the features that were used in the predictions. An obvious difference between this model and simple decision tree is that the random forest model uses all the features.\n\n```{python}\n\nrf_preds = rf_random_search.predict(x_test)\nrf_cm = confusion_matrix(y_test, rf_preds)\nrf_probs = rf_random_search.predict_proba(x_train)[:, 1]\nrf_fpr, rf_tpr, thresholds = roc_curve(y_test, rf_preds)\n\n\nresultsPlot(rf_cm, rf_fpr, rf_tpr)\n```\n\n```{python}\npd.DataFrame(\n    classification_report(y_test, rf_preds, output_dict=True)['1'],\n    index=[1]).loc[:, ['precision', 'recall', 'f1-score']]\n```\n\nThe random forest model returned an f1-score of 0.93 and an AUC score 0.94. There were six false negatives and 3 false positives. The following section compares the accuracy of all the models to decide on the best model.\n\n# Conclusion\n\nWe are able to easily compare models with the plot below. The plot below shows the mean accuracy of all iterations during the hyperparameter and training step of fitting the models along with the 95% confidence interval. Because we didn't use a randomized cross validation method to find the hyperparameters for the logistic regression model it is not included in the plot below.\n\n```{python}\nmodels = {\n    'knn': knn_random_search,\n    'dt': dt_random_search,\n    'rf': rf_random_search\n}\ndfs = []\nfor name, model in models.items():\n    this_dict = {\n        f'mean_acc': model.cv_results_['mean_test_score']\n    }\n\n    this_df = pd.DataFrame(this_dict)\n    dfs.append(this_df)\n    \nmodel_res = pd.concat(dfs)\nnested_col = [['knn']*100, ['dt']*100, ['rf']*100]\nmodel_col = [item for sublist in nested_col for item in sublist]\nmodel_res['model'] = model_col\n\n\nfinal_df = (\n    model_res\n    .groupby('model')\n    .agg(\n        mean_val=('mean_acc', 'mean'),\n        conf_int=('mean_acc', lambda x: sem(x)*1.96))\n    .reset_index()\n)\n\nsns.scatterplot(\n    final_df,\n    y='model',\n    x='mean_val'\n)\n\nfor i in range(final_df.shape[0]):\n    plt.errorbar(x=final_df['mean_val'][i], \n                 y=final_df['model'][i],\n                 xerr=final_df['conf_int'][i], \n                 fmt='o', color='black', capsize=5)\n\nplt.title('Mean Model Accuracy with Confidence Interval')\nplt.xlabel('Accuracy')\nplt.ylabel('')\nplt.yticks(range(3), labels=['Decision Tree', 'K-Nearest Neighbors', 'Random Forest'])\n\nplt.show()\n\n```\n\nAs mentioned earlier all the machine learing models performed very well with this prediction task. Admittedly this dataset is very clean and very little transformations or feature processing was needed to achieve a high model accuracy. \n\nWhen choosing a final model for prediction there are several factors that need to be considered: model accuracy, model simplicity, interpretability, and false positive vs false negative trade off. The last factor is particularly interesting because it requires some domain knowledge of the particular task at hand. Do we care more about catching everybody that has malignat breast cancer at the expense of falsely diagnosing those with benign cancer as malignant? Or do we care more about ensuring that we identify all those that are benign at the expense of misdiagnosing those with malignant breast cancer. \n\nReturning to goal of predictions like this, to diagnose breast cancer early on in its course, I believe it is more beneficial to choose a model that has a high sensitivity. This ensures that we catch as many true positive cases early on as a preliminary diagnoses. In our case the logistic regression model had the highest sensitivity (0.94) of all the models we tested. The logistic regression also has the advantage of being highly interpretable which we didn't discuss in this project because we were only concerned with predictability. My susggestion for this data would be to use a logistic regression model to predict breast cancer type.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"breast_cancer_prediction.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":"sandstone","author":"Aaron Morris","title":"Breast Cancer Classification"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}